{"cells":[{"cell_type":"markdown","metadata":{},"source":["# This notebook was created by `p2j makemore.py` followed by editing as needed."]},{"cell_type":"markdown","metadata":{},"source":["\n","<br>\n","makelogits - jos extension of makemore by Andrej Karpathy - details in ./README.md<br>\n","you give this script some words (one per line) and it will generate more things like it.<br>\n","uses super state of the art Transformer AI tech<br>\n","this code is intended to be super hackable. tune it to your needs.<br>\n","Changes from minGPT:<br>\n","- I removed the from_pretrained function where we init with GPT2 weights<br>\n","- I removed dropout layers because the models we train here are small,<br>\n","  it's not necessary to understand at this stage and at this scale.<br>\n","- I removed weight decay and all of the complexity around what parameters are<br>\n","  and are not weight decayed. I don't believe this should make a massive<br>\n","  difference at the scale that we operate on here.<br>\n"]},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[],"source":["import pdb"]},{"cell_type":"code","execution_count":41,"metadata":{},"outputs":[],"source":["import os\n","import sys\n","import time\n","import math\n","import argparse\n","from dataclasses import dataclass\n","from typing import List"]},{"cell_type":"code","execution_count":42,"metadata":{},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from torch.nn import functional as F\n","from torch.utils.tensorboard import SummaryWriter\n","from torch.utils.data.dataloader import DataLoader"]},{"cell_type":"markdown","metadata":{},"source":["-----------------------------------------------------------------------------<br>\n","JOS:"]},{"cell_type":"code","execution_count":43,"metadata":{},"outputs":[],"source":["import mambaminimal as mm # mambaminimal.py\n","# defines class Mamba"]},{"cell_type":"code","execution_count":44,"metadata":{},"outputs":[],"source":["import cProfile\n","import random"]},{"cell_type":"code","execution_count":45,"metadata":{},"outputs":[],"source":["from data_utilities import DataMode, DistanceMode, create_datasets, InfiniteDataLoader, ascii_plot"]},{"cell_type":"code","execution_count":46,"metadata":{},"outputs":[],"source":["def setSeed(seed):\n","    random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)"]},{"cell_type":"markdown","metadata":{},"source":["%%"]},{"cell_type":"markdown","metadata":{},"source":["raceTensors = True"]},{"cell_type":"code","execution_count":47,"metadata":{},"outputs":[],"source":["traceTensors = False"]},{"cell_type":"markdown","metadata":{},"source":["raceTensorsXY = True"]},{"cell_type":"code","execution_count":48,"metadata":{},"outputs":[],"source":["traceTensorsXY = False"]},{"cell_type":"markdown","metadata":{},"source":["None of these worked, but nnviz did, after creating defaultConfig below to use in \"default constructors\"<br>\n","Perhaps one or more of these can work now:<br>\n","N: from torch.utils.tensorboard import SummaryWriter<br>\n","N: import hiddenlayer as hl<br>\n","N: from ann_visualizer.visualize import ann_viz;"]},{"cell_type":"markdown","metadata":{},"source":["-----------------------------------------------------------------------------"]},{"cell_type":"markdown","metadata":{},"source":["%%"]},{"cell_type":"code","execution_count":49,"metadata":{},"outputs":[],"source":["@dataclass\n","class ModelConfig:\n","    vocab_size: int = None # number of output logits / output classes\n","    # parameters below control the sizes of each model slightly differently\n","    n_layer: int = 4\n","    n_embd: int = 64  # input embedding\n","    n_embd2: int = 64 # hidden-state embedding (GRU et al.)\n","    n_head: int = 4\n","    # extension of makemore to new types of data (beyond just words to make more of)\n","    data_mode: DataMode = DataMode.WORDS # data modes are WORDS (original), QA (Question/Answer), and DISTANCE\n","    block_size: int = 16 # input sequence length, originally max_word_length+1 == max chars/word + <start>\n","    logits_size: int = None # output logits length, originally same as block_size\n","    output_size: int = 128 # number of output logits == number of chars for WORDS, desired memory length for QA or DISTANCE\n","\n","    # block_size is important for Transformer and any model that works only on one input buffer at a time\n","    # (no recurrence)."]},{"cell_type":"markdown","metadata":{},"source":["For visualizations which need a \"default constructor\":"]},{"cell_type":"code","execution_count":50,"metadata":{},"outputs":[],"source":["defaultConfig = ModelConfig(vocab_size=27, block_size=32, logits_size=27, n_layer=4, n_head=4, n_embd=16, n_embd2=16, data_mode=DataMode.WORDS)"]},{"cell_type":"markdown","metadata":{},"source":["-----------------------------------------------------------------------------<br>\n","Transformer Language Model (*exactly* as used in GPT-2)"]},{"cell_type":"code","execution_count":51,"metadata":{},"outputs":[],"source":["class NewGELU(nn.Module):\n","    \"\"\"\n","    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT).\n","    Reference: Gaussian Error Linear Units (GELU) paper: https://arxiv.org/abs/1606.08415\n","    \"\"\"\n","    def forward(self, x):\n","        return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))"]},{"cell_type":"code","execution_count":52,"metadata":{},"outputs":[],"source":["class CausalSelfAttention(nn.Module):\n","    \"\"\"\n","    A vanilla multi-head masked self-attention layer with a projection at the end.\n","    It is possible to use torch.nn.MultiheadAttention here but I am including an\n","    explicit implementation here to show that there is nothing too scary here.\n","    \"\"\"\n","    def __init__(self, config):\n","        super().__init__()\n","        assert config.n_embd % config.n_head == 0\n","        # key, query, value projections for all heads, but in a batch\n","        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n","        # output projection\n","        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n","        # causal mask to ensure that attention is only applied to the left in the input sequence\n","        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n","                                     .view(1, 1, config.block_size, config.block_size))\n","        self.n_head = config.n_head\n","        self.n_embd = config.n_embd\n","    def forward(self, x):\n","        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n","\n","        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n","        q, k ,v  = self.c_attn(x).split(self.n_embd, dim=2)\n","        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n","        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n","        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n","\n","        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n","        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n","        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n","        att = F.softmax(att, dim=-1)\n","        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n","        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n","\n","        # output projection\n","        y = self.c_proj(y)\n","        return y"]},{"cell_type":"code","execution_count":53,"metadata":{},"outputs":[],"source":["class Block(nn.Module):\n","    \"\"\" an unassuming Transformer block \"\"\"\n","    def __init__(self, config):\n","        super().__init__()\n","        self.ln_1 = nn.LayerNorm(config.n_embd)\n","        self.attn = CausalSelfAttention(config)\n","        self.ln_2 = nn.LayerNorm(config.n_embd)\n","        self.mlp = nn.ModuleDict(dict(\n","            c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd),\n","            c_proj  = nn.Linear(4 * config.n_embd, config.n_embd),\n","            act     = NewGELU(),\n","        ))\n","        m = self.mlp\n","        self.mlpf = lambda x: m.c_proj(m.act(m.c_fc(x))) # MLP forward\n","    def forward(self, x):\n","        x = x + self.attn(self.ln_1(x))\n","        x = x + self.mlpf(self.ln_2(x))\n","        return x"]},{"cell_type":"code","execution_count":54,"metadata":{},"outputs":[],"source":["class Transformer(nn.Module):\n","    \"\"\" Transformer Language Model, exactly as seen in GPT-2 \"\"\"\n","    def __init__(self, config):\n","        super().__init__()\n","        self.block_size = config.block_size\n","        self.transformer = nn.ModuleDict(dict(\n","            wte = nn.Embedding(config.vocab_size, config.n_embd),\n","            wpe = nn.Embedding(config.block_size, config.n_embd),\n","            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n","            ln_f = nn.LayerNorm(config.n_embd),\n","        ))\n","        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n","\n","        # report number of parameters (note we don't count the decoder parameters in lm_head)\n","        n_params = sum(p.numel() for p in self.transformer.parameters())\n","        print(\"number of parameters: %.2fM\" % (n_params/1e6,))\n","    def get_block_size(self):\n","        return self.block_size\n","    def forward(self, idx, targets=None):\n","        device = idx.device\n","        b, t = idx.size()\n","        assert t <= self.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.block_size}\"\n","        pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0) # shape (1, t)\n","\n","        # forward the GPT model itself\n","        #print(f\"Transformer: idx for wte embedding =\\n{idx.transpose(0,1)=}\")\n","        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n","        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (1, t, n_embd)\n","        x = tok_emb + pos_emb\n","        for block in self.transformer.h:\n","            x = block(x)\n","        x = self.transformer.ln_f(x)\n","        logits = self.lm_head(x)\n","\n","        # if we are given some desired targets also calculate the loss\n","        loss = None\n","        if targets is not None:\n","            # print(f\"Transformer: Given {len(idx)} in idx: {idx.transpose(0,1)=}\")\n","            # print(f\"\\thave {len(targets)} targets: {targets.transpose(0,1)=}\")\n","            assert idx.shape == targets.shape, f\"Transformer: {idx.shape=} != {targets.shape=}\"\n","            logits_view = logits.view(-1, logits.size(-1))\n","            targets_view = targets.view(-1)\n","            ascii_plot(logits_view, targets_view, title=\"Transformer: Logits and Targets\")\n","            loss = F.cross_entropy(logits_view, targets_view, ignore_index=-1)\n","        return logits, loss"]},{"cell_type":"markdown","metadata":{},"source":["-----------------------------------------------------------------------------<br>\n","Bag of Words (BoW) language model"]},{"cell_type":"code","execution_count":55,"metadata":{},"outputs":[],"source":["class CausalBoW(nn.Module):\n","    \"\"\"\n","    Causal bag of words. Averages the preceding elements and looks suspiciously like\n","    a CausalAttention module you'd find in a transformer, for no apparent reason at all ;)\n","    \"\"\"\n","    def __init__(self, config):\n","        super().__init__()\n","\n","        # used to mask out vectors and preserve autoregressive property\n","        self.block_size = config.block_size\n","        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n","                            .view(1, config.block_size, config.block_size))\n","    def forward(self, x):\n","        B, T, C = x.size() # batch size, sequence length, n_embd\n","\n","        # do the weighted average of all preceeding token features\n","        att = torch.zeros((B, T, T), device=x.device)\n","        att = att.masked_fill(self.bias[:,:T,:T] == 0, float('-inf'))\n","        att = F.softmax(att, dim=-1)\n","        y = att @ x # (B, T, T) x (B, T, C) -> (B, T, C)\n","        return y"]},{"cell_type":"code","execution_count":56,"metadata":{},"outputs":[],"source":["class BoWBlock(nn.Module):\n","    \"\"\" collects BoW features and adds an MLP \"\"\"\n","    def __init__(self, config):\n","        super().__init__()\n","\n","        # Causal BoW module\n","        self.cbow = CausalBoW(config)\n","        # MLP assembler\n","        self.mlp = nn.ModuleDict(dict(\n","            c_fc    = nn.Linear(config.n_embd, config.n_embd2),\n","            c_proj  = nn.Linear(config.n_embd2, config.n_embd),\n","        ))\n","        m = self.mlp\n","        self.mlpf = lambda x: m.c_proj(F.tanh(m.c_fc(x))) # MLP forward\n","    def forward(self, x):\n","        x = x + self.cbow(x)\n","        x = x + self.mlpf(x)\n","        return x"]},{"cell_type":"code","execution_count":57,"metadata":{},"outputs":[],"source":["class BoW(nn.Module):\n","    \"\"\"\n","    takes the previous block_size tokens, encodes them with a lookup table,\n","    also encodes their positions with lookup table, then averages all of those\n","    embeddings up and uses that to predict the next token.\n","    \"\"\"\n","    def __init__(self, config):\n","        super().__init__()\n","        self.block_size = config.block_size\n","        self.vocab_size = config.vocab_size\n","        # token embedding\n","        self.wte = nn.Embedding(config.vocab_size, config.n_embd)\n","        # position embedding\n","        self.wpe = nn.Embedding(config.block_size, config.n_embd)\n","        # context block\n","        self.context_block = BoWBlock(config)\n","        # language model head decoder layer\n","        self.lm_head = nn.Linear(config.n_embd, self.vocab_size)\n","    def get_block_size(self):\n","        return self.block_size\n","    def forward(self, idx, targets=None):\n","        device = idx.device\n","        b, t = idx.size()\n","        assert t <= self.block_size, f\"BoW: Cannot forward sequence of length {t}, block size is only {self.block_size}\"\n","        pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0) # shape (1, t)\n","\n","        # forward the token and position embedding layers\n","        tok_emb = self.wte(idx) # token embeddings of shape (b, t, n_embd)\n","        pos_emb = self.wpe(pos) # position embeddings of shape (1, t, n_embd)\n","        # add and run through the decoder MLP\n","        x = tok_emb + pos_emb\n","        # run the bag of words context module\n","        x = self.context_block(x)\n","        # decode to next token probability\n","        logits = self.lm_head(x) # vocab_size\n","\n","        # if we are given some desired targets also calculate the loss\n","        loss = None\n","        if targets is not None:\n","            logits_view = logits.view(-1, logits.size(-1))\n","            targets_view = targets.view(-1)\n","            loss = F.cross_entropy(logits_view, targets_view, ignore_index=-1)\n","            ascii_plot(logits_view, targets_view, title=\"BoW: Logits and Targets\")\n","        return logits, loss"]},{"cell_type":"markdown","metadata":{},"source":["-----------------------------------------------------------------------------"]},{"cell_type":"markdown","metadata":{},"source":["\n","<br>\n","Recurrent Neural Net language model: either a vanilla RNN recurrence or a GRU.<br>\n","Did not implement an LSTM because its API is a bit more annoying as it has<br>\n","both a hidden state and a cell state, but it's very similar to GRU and in<br>\n","practice works just as well.<br>\n"]},{"cell_type":"code","execution_count":58,"metadata":{},"outputs":[],"source":["class RNNCell(nn.Module):\n","    \"\"\"\n","    the job of a 'Cell' is to:\n","    take input at current time step x_{t} and the hidden state at the\n","    previous time step h_{t-1} and return the resulting hidden state\n","    h_{t} at the current timestep\n","    \"\"\"\n","    def __init__(self, config=defaultConfig):\n","        super().__init__()\n","        self.xh_to_h = nn.Linear(config.n_embd + config.n_embd2, config.n_embd2)\n","    def forward(self, xt, hprev):\n","        xh = torch.cat([xt, hprev], dim=1)\n","        ht = F.tanh(self.xh_to_h(xh))\n","        return ht"]},{"cell_type":"code","execution_count":59,"metadata":{},"outputs":[],"source":["class GRUCell(nn.Module):\n","    \"\"\"\n","    same job as RNN cell, but a bit more complicated recurrence formula\n","    that makes the GRU more expressive and easier to optimize.\n","    \"\"\"\n","    def __init__(self, config):\n","        super().__init__()\n","        # input, forget, output, gate\n","        self.xh_to_z = nn.Linear(config.n_embd + config.n_embd2, config.n_embd2)\n","        self.xh_to_r = nn.Linear(config.n_embd + config.n_embd2, config.n_embd2)\n","        self.xh_to_hbar = nn.Linear(config.n_embd + config.n_embd2, config.n_embd2)\n","    def forward(self, xt, hprev):\n","        # first use the reset gate to wipe some channels of the hidden state to zero\n","        xh = torch.cat([xt, hprev], dim=1)\n","        r = F.sigmoid(self.xh_to_r(xh))\n","        hprev_reset = r * hprev\n","        # calculate the candidate new hidden state hbar\n","        xhr = torch.cat([xt, hprev_reset], dim=1)\n","        hbar = F.tanh(self.xh_to_hbar(xhr))\n","        # calculate the switch gate that determines if each channel should be updated at all\n","        z = F.sigmoid(self.xh_to_z(xh))\n","        # blend the previous hidden state and the new candidate hidden state\n","        ht = (1 - z) * hprev + z * hbar\n","        return ht"]},{"cell_type":"code","execution_count":60,"metadata":{},"outputs":[],"source":["class RNN(nn.Module):\n","    def __init__(self, config=defaultConfig, cell_type='gru'):\n","        super().__init__()\n","        self.block_size = config.block_size # in\n","        self.logits_size = config.logits_size # out\n","        self.vocab_size = config.vocab_size # embedding size\n","        self.start = nn.Parameter(torch.zeros(1, config.n_embd2)) # the starting hidden state\n","        self.wte = nn.Embedding(config.vocab_size+1, config.n_embd) # token embeddings table, +1 for NULL\n","        if traceTensors:\n","            print(f\"\\nRNN: token embedding shape is num_tokens x n_embd: {(config.vocab_size+1)=} by {config.n_embd=}\")\n","        if cell_type == 'rnn':\n","            self.cell = RNNCell(config)\n","        elif cell_type == 'gru':\n","            self.cell = GRUCell(config)\n","        self.lm_head = nn.Linear(config.n_embd2, self.logits_size)\n","    def get_block_size(self):\n","        return self.block_size\n","    def forward(self, tokens, targets=None):\n","        device = tokens.device\n","        b, t = tokens.size()\n","\n","        # print(f\"RNN: tokens shape is {tokens.shape}\")\n","        # Not true for last block: assert t == self.block_size, f\"RNN: {t=} != {self.block_size=}\"\n","\n","        # pdb.set_trace()\n","\n","        # embed all the integers up front and all at once for efficiency\n","        if traceTensors:\n","            print(f\"\\nRNN: === AT DATA EMBEDDING BREAKPOINT:\\n{tokens=}\")\n","        emb = self.wte(tokens) # (b, t, n_embd)\n","\n","        # sequentially iterate over the inputs and update the RNN state each tick\n","        hprev = self.start.expand((b, -1)) # expand out the batch dimension\n","        hiddens = []\n","        for i in range(t):\n","            xt = emb[:, i, :] # (b, n_embd)\n","            ht = self.cell(xt, hprev) # (b, n_embd2)\n","            hprev = ht\n","            hiddens.append(ht)\n","\n","        # decode the outputs\n","        hidden = torch.stack(hiddens, 1) # (b, t, n_embd2)\n","        logits = self.lm_head(hidden)\n","\n","        # if we are given some desired targets also calculate the loss\n","        loss = None\n","        if targets is not None:\n","            # Not very interesting since RNNs must be called one sample at a time\n","            # print(f\"RNN: Given {len(tokens)} in tokens: {tokens.transpose(0,1)=}\")\n","            # print(f\"\\thave {len(targets)} targets: {targets.transpose(0,1)=}\")\n","            assert tokens.shape == targets.shape, f\"RNN: {tokens.shape=} != {targets.shape=}\"\n","            # print(f\"{config.logits_size=}\")\n","            # print(f\"{logits.shape=}\")\n","            # Clip targets to ensure they are within the valid range [0, C-1]\n","            num_classes = logits.size(-1)\n","            targets_clipped = torch.clamp(targets, 0, num_classes - 1)\n","            logits_view = logits.view(-1, logits.size(-1))\n","            targets_view = targets_clipped.view(-1)\n","            ascii_plot(logits_view, targets_view, title=\"RNN: Logits and Targets\")\n","            loss = F.cross_entropy(logits_view, targets_view, ignore_index=-1)\n","            # print(f\"RNN: loss={loss}, {logits.shape=}\")\n","        return logits, loss"]},{"cell_type":"markdown","metadata":{},"source":["-----------------------------------------------------------------------------<br>\n","MLP language model"]},{"cell_type":"code","execution_count":61,"metadata":{},"outputs":[],"source":["class MLP(nn.Module):\n","    \"\"\"\n","    takes the previous block_size tokens, encodes them with a lookup table,\n","    concatenates the vectors and predicts the next token with an MLP.\n","    Reference:\n","    Bengio et al. 2003 https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf\n","    \"\"\"\n","    def __init__(self, config=defaultConfig):\n","        super().__init__()\n","        self.block_size = config.block_size\n","        self.vocab_size = config.vocab_size\n","        self.wte = nn.Embedding(config.vocab_size + 1, config.n_embd) # token embeddings table\n","        # +1 in the line above for a special <BLANK> token that gets inserted if encoding a token\n","        # before the beginning of the input sequence\n","        self.mlp = nn.Sequential(\n","            nn.Linear(self.block_size * config.n_embd, config.n_embd2),\n","            nn.Tanh(),\n","            nn.Linear(config.n_embd2, self.vocab_size)\n","        )\n","    def get_block_size(self):\n","        return self.block_size\n","    def forward(self, idx, targets=None):\n","\n","        # gather the word embeddings of the previous 3 words\n","        embs = []\n","        for k in range(self.block_size):\n","            tok_emb = self.wte(idx) # token embeddings of shape (b, t, n_embd)\n","            idx = torch.roll(idx, 1, 1)\n","            idx[:, 0] = self.vocab_size # special <BLANK> token\n","            embs.append(tok_emb)\n","\n","        # concat all of the embeddings together and pass through an MLP\n","        x = torch.cat(embs, -1) # (b, t, n_embd * block_size)\n","        logits = self.mlp(x)\n","\n","        # if we are given some desired targets also calculate the loss\n","        loss = None\n","        if targets is not None:\n","            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n","        return logits, loss"]},{"cell_type":"markdown","metadata":{},"source":["-----------------------------------------------------------------------------<br>\n","Bigram language model"]},{"cell_type":"code","execution_count":62,"metadata":{},"outputs":[],"source":["class Bigram(nn.Module):\n","    \"\"\"\n","    Bigram Language Model 'neural net', simply a lookup table of logits for the\n","    next character given a previous character.\n","    \"\"\"\n","    def __init__(self, config=defaultConfig):\n","        super().__init__()\n","        n = config.vocab_size\n","        self.logits = nn.Parameter(torch.zeros((n, n)))\n","    def get_block_size(self):\n","        return 1 # this model only needs one previous character to predict the next\n","    def forward(self, idx, targets=None):\n","         # 'forward pass', lol\n","        logits = self.logits[idx]\n","\n","        # if we are given some desired targets also calculate the loss\n","        loss = None\n","        if targets is not None:\n","            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n","        return logits, loss"]},{"cell_type":"markdown","metadata":{},"source":["-----------------------------------------------------------------------------<br>\n","helper functions for evaluating and sampling from the model"]},{"cell_type":"code","execution_count":63,"metadata":{},"outputs":[],"source":["@torch.no_grad()\n","def generate(model, idx, max_new_tokens, temperature=1.0, do_sample=False, top_k=None):\n","    \"\"\"\n","    Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n","    the sequence max_new_tokens times, feeding the predictions back into the model each time.\n","    A list of max_new_tokens tokens is returned.\n","    Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n","    \"\"\"\n","    block_size = model.get_block_size()\n","    for _ in range(max_new_tokens):\n","        # if the sequence context is growing too long we must crop it at block_size\n","        idx_cond = idx if idx.size(1) <= block_size else idx[:, -block_size:]\n","        # forward the model to get the logits for the index in the sequence\n","        logits, _ = model(idx_cond)\n","        if traceTensors:\n","            print(f\"generate:\\n\\t{idx_cond.shape=}\\n\\t{logits.shape=}\")\n","        # pluck the logits (b, t, d) at the final step and scale by desired temperature\n","        logits = logits[:, -1, :] / max(temperature, 1.0e-7)\n","        # optionally crop the logits to only the top k options\n","        if top_k is not None:\n","            v, _ = torch.topk(logits, top_k)\n","            logits[logits < v[:, [-1]]] = -float('Inf')\n","        # apply softmax to convert logits to (normalized) probabilities\n","        probs = F.softmax(logits, dim=-1)\n","        # either sample from the distribution or take the most likely element\n","        if do_sample:\n","            idx_next = torch.multinomial(probs, num_samples=1)\n","        else:\n","            _, idx_next = torch.topk(probs, k=1, dim=-1)\n","        # append sampled index to the running sequence and continue\n","        idx = torch.cat((idx, idx_next), dim=1)\n","    return idx"]},{"cell_type":"code","execution_count":64,"metadata":{},"outputs":[],"source":["@torch.no_grad()\n","def print_word_samples(num=10):\n","    \"\"\" samples from the model and pretty prints the decoded samples \"\"\"\n","    setSeed(43)\n","    X_init = torch.zeros(num, 1, dtype=torch.long).to(args.device) # generate num examples in parallel as one batch\n","    top_k = args.top_k if args.top_k != -1 else None\n","    steps = config.output_size - 1 # == max_word_length (-1 because we added 1 for <START> token (index 0))\n","    X_samp = generate(model, X_init, steps, top_k=top_k, do_sample=True).to('cpu')\n","    train_samples, test_samples, new_samples = [], [], []\n","    assert X_samp.size(0) == num, f\"I thought {num=} would equal {X_samp.size(0)=}\"\n","    for i in range(X_samp.size(0)): # loop over generated samples == batch size\n","        # get the i'th row of sampled integers, as a python list:\n","        row = X_samp[i, 1:].tolist() # initial <START> token omitted\n","        crop_index = row.index(0) if 0 in row else len(row) # find the <STOP> token\n","        row = row[:crop_index] # take everything up to but not including <STOP> token\n","        word_samp = train_dataset.decode(row) # convert the list of integers to a string\n","        # separately track samples that we have and have not seen before\n","        if train_dataset.contains(word_samp):\n","            train_samples.append(word_samp)\n","        elif test_dataset.contains(word_samp):\n","            test_samples.append(word_samp)\n","        else:\n","            new_samples.append(word_samp)\n","    print('-'*80)\n","    for lst, desc in [(train_samples, 'in train'), (test_samples, 'in test'), (new_samples, 'new')]:\n","        print(f\"{len(lst)} samples that are {desc}:\")\n","        for word in lst:\n","            print(word)\n","    print('-'*80)"]},{"cell_type":"code","execution_count":65,"metadata":{},"outputs":[],"source":["@torch.inference_mode()\n","def evaluate(model, dataset, data_mode, batch_size=50, max_batches=None, make_graphs=False, num_print=0):\n","    model.eval() # set evaluation mode\n","\n","    # Output model diagram if requested:\n","    if make_graphs:\n","        # nnviz on the command line works. These don't:\n","        # ann_viz(model)\n","        trySummaryWriter = False\n","        if trySummaryWriter:\n","            writer = SummaryWriter()\n","            dummy_input = torch.randn(1, len(dataset), batch_size)\n","            writer.add_graph(model, dummy_input)\n","            writer.close()\n","        # tryHiddenLayer = False\n","        # if tryHiddenLayer:\n","        #     hl_graph = hl.build_graph(model, test_dataset)  # Adjust the input shape\n","        #     hl_graph.theme = hl.graph.THEMES[\"blue\"].copy()\n","        #     gfname = \"model_visualization\"\n","        #     hl_graph.save(gfname, format=\"png\")\n","        #     print(f\"Written: {gfname}.png\")\n","\n","    # original: loader = DataLoader(dataset, shuffle=True, batch_size=batch_size, num_workers=0)\n","    doShuffle = (data_mode != DataMode.DISTANCE) # this is a memory task that shuffling would destroy\n","    loader = DataLoader(dataset, shuffle=doShuffle, batch_size=batch_size, num_workers=0) # , batch_sampler=doShuffle)\n","    loader = DataLoader(dataset, shuffle=doShuffle, batch_size=batch_size, num_workers=0) # , batch_sampler=doShuffle)\n","    losses = []\n","    for i, batch in enumerate(loader):\n","        batch = [t.to(args.device) for t in batch]\n","        X, Y = batch\n","        if traceTensorsXY:\n","            print(f\"evaluate: {X.shape=}\\n{Y.shape=}\")\n","        logits, loss = model(X, Y) # ********************** MAIN EVENT **********************\n","        if traceTensorsXY:\n","            print(f\"evaluate: {logits.shape=}\")\n","        losses.append(loss.item())\n","        if num_print>0:\n","            final_logits = logits[:, -1, :] # b t v -> b v where v = vocab_size or num-logits\n","            probs = F.softmax(final_logits, dim=-1) # logits to probabilities\n","            _, idx_best = torch.topk(probs, k=1, dim=-1) # find the index of maximum probability in each batch element\n","            Yh = idx_best\n","            for p in range(min(num_print,batch_size)):\n","                bp = p + i*batch_size\n","                if traceTensorsXY:\n","                    print(f\"\\nevaluate:\\nX[{bp}]:\\n{X}\\nY[{bp}]:\\n{Y}\\nYh[{bp}]:{Yh}\")\n","            num_print -= batch_size\n","        if max_batches is not None and i >= max_batches:\n","            break\n","    mean_loss = torch.tensor(losses).mean().item()\n","    model.train() # reset model back to training mode\n","    return mean_loss"]},{"cell_type":"markdown","metadata":{},"source":["-----------------------------------------------------------------------------"]},{"cell_type":"markdown","metadata":{},"source":["%%"]},{"cell_type":"markdown","metadata":{},"source":["We are not a module, so either in Jupyter or standalone Python"]},{"cell_type":"code","execution_count":66,"metadata":{},"outputs":[],"source":["# print(f\"=== {__name__}({__file__}):\")"]},{"cell_type":"markdown","metadata":{},"source":["failed to work: assume_jupyter = __name__ != '__main__'"]},{"cell_type":"code","execution_count":68,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["{'input_file': None, 'work_dir': 'out', 'data_mode': 'words', 'block_size': -1, 'embedding_size': 32, 'logits_size': None, 'device': 'cpu', 'resume': False, 'num_workers': 4, 'max_steps': 210, 'seed': 3407, 'sample_only': False, 'top_k': -1, 'type': 'transformer', 'n_layer': 4, 'n_head': 4, 'n_embd': 64, 'n_embd2': 64, 'batch_size': 1, 'learning_rate': 0.0005, 'weight_decay': 0.01}\n"]}],"source":["if True: # __name__ == '__main__':\n","    # parse command line args\n","    parser = argparse.ArgumentParser(description=\"Make More\")\n","    # system/input/output\n","    parser.add_argument('--input-file', '-i', type=str, default=None, help=\"input text file, where .txt suffix => one word per line to make more of, while .tsv => <answer><tab><prompt> each line (e.g., ListOps data)\")\n","    parser.add_argument('--work-dir', '-o', type=str, default='out', help=\"output working directory\")\n","    parser.add_argument('--data-mode', type=str, default=\"words\", help=\"data type: (words|qa|distance|distance-exp)\")\n","    # input/output sizes and dimensionalities\n","    parser.add_argument('--block-size', type=int, default=-1, help=\"input block size [default = vocab_size measured from data]: (max word length + 1 | max Q+A length + 2 | max short-term memory\")\n","    parser.add_argument('--embedding-size', type=int, default=32, help=\"embedding size: (max word length + 1 | number of Answers | max_distance + 1)\")\n","    parser.add_argument('--logits-size', type=int, default=None, help=\"logits size: defaults to embedding-size\")\n","    # training\n","    parser.add_argument('--device', type=str, default='cpu', help=\"device to use for compute, examples: cpu|cuda|cuda:2|mps\")\n","    parser.add_argument('--resume', action='store_true', help=\"when this flag is used, we will resume optimization from existing model in the workdir\")\n","    parser.add_argument('--num-workers', '-n', type=int, default=4, help=\"number of data workers for both train/test\")\n","    parser.add_argument('--max-steps', type=int, default=-1, help=\"max number of optimization steps to run for, or -1 for infinite.\")\n","    # sampling\n","    parser.add_argument('--seed', type=int, default=3407, help=\"seed\")\n","    parser.add_argument('--sample-only', action='store_true', help=\"just sample from the model and quit, don't train\")\n","    parser.add_argument('--top-k', type=int, default=-1, help=\"top-k for sampling, -1 means no top-k\")\n","    # model\n","    parser.add_argument('--type', type=str, default='transformer', help=\"model class type to use, bigram|mlp|rnn|gru|bow|transformer|mamba\")\n","    parser.add_argument('--n-layer', type=int, default=4, help=\"number of layers\")\n","    parser.add_argument('--n-head', type=int, default=4, help=\"number of heads (in a transformer)\")\n","    parser.add_argument('--n-embd', type=int, default=64, help=\"number of feature channels in the model\")\n","    parser.add_argument('--n-embd2', type=int, default=64, help=\"number of feature channels elsewhere in the model\")\n","    # optimization\n","    parser.add_argument('--batch-size', '-b', type=int, default=1, help=\"batch size during optimization\")\n","    parser.add_argument('--learning-rate', '-l', type=float, default=5e-4, help=\"learning rate\")\n","    parser.add_argument('--weight-decay', '-w', type=float, default=0.01, help=\"weight decay\")\n","   \n","    if True: # args == None: # was the failed flag assume_jupyter:\n","        args_list = \"--data-mode words --max-steps 210\".split() # test example - EDIT THIS\n","        args = parser.parse_args(args_list)\n","    # else:\n","    #    args = parser.parse_args()\n","    print(vars(args))\n"]},{"cell_type":"code","execution_count":73,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["data_mode=<DataMode.WORDS: 1>\n","block_size=None\n","create_words_datasets: computed block_size=16\n","train_dataset says vocab_size=27\n","main: logits_size set to vocab_size=27\n"]}],"source":["# system inits\n","if True: # keep indentation same as .py version\n","    torch.manual_seed(args.seed)\n","    torch.cuda.manual_seed_all(args.seed)\n","    os.makedirs(args.work_dir, exist_ok=True)\n","    writer = SummaryWriter(log_dir=args.work_dir)\n","    def str2dm(s):\n","        if s == \"words\":\n","            dm = DataMode.WORDS\n","        elif s == \"qa\":\n","            dm = DataMode.QA\n","        elif s == \"distance\":\n","            dm = DataMode.DISTANCE\n","        else:\n","            assert False, f\"Unrecognized --data-mode {s}\"\n","        return dm\n","    data_mode = str2dm(args.data_mode)\n","    print(f\"{data_mode=}\")\n","    input_file = args.input_file\n","    if input_file == None:\n","        match data_mode:\n","            case DataMode.WORDS:\n","                input_file = './data/words/names.txt'\n","            case DataMode.QA:\n","                input_file = './data/listops/data.txt'\n","            case DataMode.DISTANCE:\n","                input_file = './data/distance/dist1.txt'\n","\n","    # init datasets\n","    block_size = None if args.block_size <= 0 else args.block_size\n","    print(f\"{block_size=}\")\n","    train_dataset, test_dataset, block_size = create_datasets(input_file, data_mode, block_size)\n","    embedding_size = args.embedding_size\n","    logits_size = args.logits_size\n","    vocab_size = train_dataset.get_vocab_size()\n","    print(f\"train_dataset says {vocab_size=}\")\n","    if logits_size == None:\n","        logits_size = vocab_size\n","        print(f\"main: logits_size set to {vocab_size=}\")"]},{"cell_type":"markdown","metadata":{},"source":["   if data_mode == DataMode.DISTANCE:<br>\n","       maxLastOccurrence = max(Y)<br>\n","       if max(train_dataset(:,"]},{"cell_type":"code","execution_count":74,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+++ dataset determined that: vocab_size=27\n","test_dataset: test_dataset[0]=(tensor([ 0, 11, 15, 22,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]), tensor([11, 15, 22,  1,  0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]))\n","main: vocab_size=27\n","number of parameters: 0.20M\n","model #params: 204544\n"]},{"name":"stderr","output_type":"stream","text":["/Users/jos/miniforge3/envs/env_pytorch/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]},{"name":"stdout","output_type":"stream","text":["shuffle=True\n","Transformer: Logits and Targets\n","===============================\n","Batch Sample 0:\n","Class 0: [ ######### (0.03)]\n","Class 1: [ # (0.00)]\n","Class 2: [ ########## (0.03)]\n","Class 3: [ ###### (0.02)]\n","Class 4: [ ############### (0.05)]\n","Class 5: [ #### (0.01)]\n","Class 6: [ ######### (0.03)]\n","Class 7: [*########### (0.04)]\n","Class 8: [ ########### (0.03)]\n","Class 9: [ #################### (0.06)]\n","Class 10: [ ######### (0.03)]\n","Class 11: [ ########### (0.03)]\n","Class 12: [ ######################### (0.08)]\n","Class 13: [ ####### (0.02)]\n","Class 14: [ ######## (0.03)]\n","Class 15: [ ############# (0.04)]\n","Class 16: [ ####################### (0.07)]\n","Class 17: [ #### (0.01)]\n","Class 18: [ ######### (0.03)]\n","Class 19: [ ############### (0.05)]\n","Class 20: [ ########## (0.03)]\n","Class 21: [ ########### (0.04)]\n","Class 22: [ ####### (0.02)]\n","Class 23: [ #################### (0.06)]\n","Class 24: [ ########## (0.03)]\n","Class 25: [ ################################## (0.10)]\n","Class 26: [ ######## (0.02)]\n","------------------------------------------------------------\n","step 0 | loss 3.7577 | step time 191.20ms\n","step 10 | loss 3.5376 | step time 6.29ms\n","step 20 | loss 2.9170 | step time 5.87ms\n","step 30 | loss 3.3919 | step time 6.17ms\n","step 40 | loss 3.2290 | step time 6.06ms\n","step 50 | loss 2.4786 | step time 5.90ms\n","step 60 | loss 3.1828 | step time 5.82ms\n","step 70 | loss 2.8215 | step time 5.97ms\n","step 80 | loss 2.8313 | step time 6.08ms\n","step 90 | loss 2.5240 | step time 6.22ms\n","Transformer: Logits and Targets\n","===============================\n","Batch Sample 0:\n","Class 0: [ # (0.01)]\n","Class 1: [ ############################ (0.20)]\n","Class 2: [ #### (0.03)]\n","Class 3: [ #### (0.03)]\n","Class 4: [ ####### (0.05)]\n","Class 5: [ ######## (0.06)]\n","Class 6: [ # (0.01)]\n","Class 7: [ ### (0.02)]\n","Class 8: [ #### (0.03)]\n","Class 9: [ ######## (0.06)]\n","Class 10: [ ##### (0.04)]\n","Class 11: [ ####### (0.06)]\n","Class 12: [ ##### (0.04)]\n","Class 13: [ ######## (0.06)]\n","Class 14: [ #### (0.03)]\n","Class 15: [ ## (0.02)]\n","Class 16: [ # (0.01)]\n","Class 17: [ # (0.01)]\n","Class 18: [*########## (0.08)]\n","Class 19: [ ### (0.03)]\n","Class 20: [ # (0.01)]\n","Class 21: [ # (0.01)]\n","Class 22: [ ## (0.02)]\n","Class 23: [  (0.01)]\n","Class 24: [ # (0.01)]\n","Class 25: [ ## (0.02)]\n","Class 26: [ ##### (0.04)]\n","------------------------------------------------------------\n","step 100 | loss 2.6091 | step time 6.49ms\n","step 110 | loss 2.5073 | step time 5.77ms\n","step 120 | loss 3.0987 | step time 8.79ms\n","step 130 | loss 2.4516 | step time 5.98ms\n","step 140 | loss 2.0804 | step time 6.01ms\n","step 150 | loss 2.3911 | step time 5.94ms\n","step 160 | loss 2.9544 | step time 5.91ms\n","step 170 | loss 1.7850 | step time 6.00ms\n","step 180 | loss 2.6121 | step time 6.04ms\n","step 190 | loss 2.6791 | step time 6.18ms\n","Transformer: Logits and Targets\n","===============================\n","Batch Sample 0:\n","Class 0: [  (0.00)]\n","Class 1: [ ##### (0.04)]\n","Class 2: [ ####### (0.06)]\n","Class 3: [ #### (0.04)]\n","Class 4: [ ## (0.02)]\n","Class 5: [ ## (0.02)]\n","Class 6: [ ## (0.02)]\n","Class 7: [ ##### (0.04)]\n","Class 8: [ ########## (0.08)]\n","Class 9: [ ## (0.02)]\n","Class 10: [ ############## (0.11)]\n","Class 11: [ ################# (0.13)]\n","Class 12: [ ## (0.02)]\n","Class 13: [ ####### (0.06)]\n","Class 14: [ ### (0.03)]\n","Class 15: [ ## (0.02)]\n","Class 16: [ ### (0.03)]\n","Class 17: [ # (0.01)]\n","Class 18: [*##### (0.04)]\n","Class 19: [ ### (0.03)]\n","Class 20: [ #### (0.03)]\n","Class 21: [ # (0.01)]\n","Class 22: [ ## (0.02)]\n","Class 23: [ ## (0.02)]\n","Class 24: [ # (0.01)]\n","Class 25: [ ## (0.02)]\n","Class 26: [ ####### (0.06)]\n","------------------------------------------------------------\n","step 200 | loss 2.4230 | step time 6.55ms\n","step 200 train loss: 2.6709330081939697 test loss: 2.4378116130828857\n","test loss 2.4378116130828857 is the best so far, saving model to out/model.pt\n","--------------------------------------------------------------------------------\n","3 samples that are in train:\n","brom\n","zo\n","vann\n","0 samples that are in test:\n","13 samples that are new:\n","renyel\n","kan\n","fairosm\n","zetieqsm\n","banna\n","wjhi\n","iunrio\n","kleubpa\n","ansu\n","ckaa\n","cbsy\n","mifsown\n","pi\n","--------------------------------------------------------------------------------\n"]}],"source":["if True:\n","    print(f\"+++ dataset determined that: {vocab_size=}\")\n","    print(f\"test_dataset: {test_dataset[0]=}\")\n","    # init model - FIXME: For DataMode.DISTANCE, output an unsigned int instead of (too many) logits\n","    config = ModelConfig(vocab_size=vocab_size, block_size=block_size, logits_size=logits_size,\n","                         n_layer=args.n_layer, n_head=args.n_head,\n","                         n_embd=args.n_embd, n_embd2=args.n_embd2, data_mode=data_mode)\n","    print(f\"main: {vocab_size=}\")\n","    if args.type == 'transformer':\n","        model = Transformer(config)\n","    elif args.type == 'bigram':\n","        model = Bigram(config)\n","    elif args.type == 'mlp':\n","        model = MLP(config)\n","    elif args.type == 'rnn':\n","        model = RNN(config, cell_type='rnn')\n","    elif args.type == 'gru':\n","        model = RNN(config, cell_type='gru')\n","    elif args.type == 'bow':\n","        model = BoW(config)\n","    elif args.type == 'mamba':\n","        mambaConfig = mm.ModelArgs(\n","            d_model=args.n_embd,\n","            n_layer=args.n_layer,\n","            vocab_size=vocab_size,\n","            block_size=block_size,\n","            # Mamba output size == block_size because it is a sequence to sequence map:\n","            # Mamba logits size == vocab_size\n","            d_state=args.n_head, # too janky?\n","            expand=2, # FIXME: bring out state-expansion-factor parameter\n","            dt_rank='auto', # auto => d_model/16\n","            d_conv=4, # Conv1d kernel size\n","            pad_vocab_size_multiple=1, # Forces vocab_size to be a multiple of this\n","            conv_bias=True,\n","            bias=False)\n","        model = mm.Mamba(mambaConfig)\n","    else:\n","        raise ValueError(f'model type {args.type} is not recognized')\n","    model.to(args.device)\n","    print(f\"model #params: {sum(p.numel() for p in model.parameters())}\")\n","    if args.resume or args.sample_only: # note: if we sample-only then we also assume we are resuming\n","        print(\"resuming from existing model in the workdir\")\n","        model.load_state_dict(torch.load(os.path.join(args.work_dir, 'model.pt')))\n","    if args.sample_only:\n","        print_word_samples(block_size,data_mode)\n","        sys.exit()\n","\n","    # init optimizer\n","    optimizer = torch.optim.AdamW(model.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay, betas=(0.9, 0.99), eps=1e-8)\n","\n","    # init dataloader\n","    shuffle = (data_mode != DataMode.DISTANCE) # this is a memory task that shuffling would destroy\n","    batch_loader = InfiniteDataLoader(train_dataset, shuffle, batch_size=args.batch_size, pin_memory=True, num_workers=args.num_workers)\n","\n","    # training loop\n","    best_loss = None\n","    step = 0\n","    while True:\n","        t0 = time.time()\n","\n","        # print(\"=== AT TRAINING BREAKPOINT ===\")\n","        # pdb.set_trace()\n","\n","        # get the next batch, ship to device, and unpack it to input and target\n","        batch = batch_loader.next()\n","        batch = [t.to(args.device) for t in batch] # for each tensor t in batch\n","        X, Y = batch\n","        assert X.shape == Y.shape, f\"{X.shape=} != {Y.shape=}\"\n","        if traceTensors:\n","            print(f\"\\nbatch is type {type(batch)} with length {len(batch)}\")\n","            print(f\"{X.shape=}, {Y.shape=}\")\n","            print(f\"X:\\n\\t{X=}\")\n","            print(f\"Y:\\n\\t{Y=}\")\n","\n","        # feed into the model\n","        logits, loss = model(X, Y)\n","\n","        # calculate the gradient, update the weights\n","        model.zero_grad(set_to_none=True)\n","        loss.backward()\n","        optimizer.step()\n","\n","        # wait for all CUDA work on the GPU to finish then calculate iteration time taken\n","        if args.device.startswith('cuda'):\n","            torch.cuda.synchronize()\n","        t1 = time.time()\n","\n","        # logging\n","        if step % 10 == 0:\n","            print(f\"step {step} | loss {loss.item():.4f} | step time {(t1-t0)*1000:.2f}ms\")\n","\n","        # graph the model: evaluate(model, test_dataset, data_mode, batch_size=args.batch_size, max_batches=1, make_graphs=True)\n","\n","        # evaluate the model\n","        if step > 0 and step % 200 == 0:\n","            # print(\"\\n\"+'-'*30+\" TRAIN \"+'-'*30)\n","            train_loss = evaluate(model, train_dataset, data_mode, batch_size=args.batch_size, max_batches=10, num_print=10)\n","            # print(\"\\n\"+'-'*30+\" TEST \"+'-'*30)\n","            test_loss  = evaluate(model, test_dataset, data_mode, batch_size=args.batch_size, max_batches=10, num_print=10)\n","            writer.add_scalar(\"Loss/train\", train_loss, step)\n","            writer.add_scalar(\"Loss/test\", test_loss, step)\n","            writer.flush()\n","            print(f\"step {step} train loss: {train_loss} test loss: {test_loss}\")\n","            # save the model to disk if it has improved\n","            if best_loss is None or test_loss < best_loss:\n","                out_path = os.path.join(args.work_dir, \"model.pt\")\n","                print(f\"test loss {test_loss} is the best so far, saving model to {out_path}\")\n","                torch.save(model.state_dict(), out_path)\n","                best_loss = test_loss\n","        if data_mode == DataMode.WORDS:\n","            # sample words from the model\n","            if step > 0 and step % 200 == 0:\n","                print_word_samples(block_size)\n","        step += 1\n","        # termination conditions\n","        if args.max_steps >= 0 and step >= args.max_steps:\n","            break"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":2}
